# -*- coding: utf-8 -*-
"""indian_pines_pca_knn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F6cNtPe-M7a7z214sEpesuFv1mDepkzq
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn import decomposition
from sklearn import datasets

!pip install hvplot
import hvplot
import hvplot.pandas

pd.options.plotting.backend = 'hvplot'

df = pd.read_csv("https://raw.githubusercontent.com/syamkakarla98/Dimensionality-reduction-and-classification-on-Hyperspectral-Images-Using-Python/master/Complete_Data_.csv")
df.head()

from sklearn.preprocessing import StandardScaler
ind=[]
for i in range(200):
    ind.append('px'+str(i+1))

features = ind
x = df.loc[:, features].values
# Separating out the target
y = df.loc[:,['target']].values
# Standardizing the features
from sklearn.preprocessing import MinMaxScaler
scaler_model = MinMaxScaler()
scaler_model.fit(x.astype(float))
x=scaler_model.transform(x)

##from sklearn.decomposition import PCA

## Finding the principle components
##pca = PCA(n_components=10)
##principalComponents = pca.fit_transform(x)
##ev=pca.explained_variance_ratio_

pca = PCA(n_components = 10)
dt = pca.fit_transform(df.iloc[:, :-1].values)
q = pd.concat([pd.DataFrame(data = dt), pd.DataFrame(data = y.ravel())], axis = 1)
q.columns = [f'PC-{i}' for i in range(1,11)]+['class']

print(q)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
q, q['class'], test_size = 0.3, random_state = 100)
y_train = y_train.ravel()
y_test = y_test.ravel()

from sklearn.neighbors import KNeighborsClassifier  
from sklearn import metrics
import time

#iniciando o KNN Classificador
model = KNeighborsClassifier()
model = KNeighborsClassifier(n_neighbors = 2, weights='uniform', algorithm='auto')
model.fit(X_train, y_train)
start = time.time()
Yhat = model.predict(X_test)
end = time.time()
print('Time Taken For Classification is :',(end - start))
print("Accuracy :",metrics.accuracy_score(Yhat, y_test)*100)
print('\n','*'*11,'Accuracy of INDIAN-PINES Dataset Before Redutor','*'*11)
print('*'*11,' Classifier : K-NEAREST NEIGHBOUR ','*'*11)
ks = []
i=0
start = time.time()
for K in range(100, 1000, 5):
 K_value = K+1
 neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')
 neigh.fit(X_train, y_train) 
 y_pred = neigh.predict(X_test)
 ks.append(metrics.accuracy_score(y_test,y_pred)*100)
 i=i+1
 print ("#%d"%(i),"Accuracy is :%1.10f"%(metrics.accuracy_score(y_test,y_pred)*100),"% ","for K-Value: %4d"%(K_value))
 end = time.time()
print(end-start)

import numpy as np
print(np.mean(ks))

plt.bar([1,2,3,4,5,6,7,8,9,10],list(ev*100),label='Principal Components',color='b')
plt.legend()
plt.xlabel('Principal Components')
pc=[]
for i in range(10):
    pc.append('PC'+str(i+1))
#plt.xticks([1,2,3,4,5,6,7,8,9,10],pc, fontsize=8, rotation=30)
plt.xticks([1,2,3,4,5,6,7,8,9,10],pc, fontsize=8, rotation=30)
plt.ylabel('Variance Ratio')
plt.title('Variance Ratio of INDIAN PINES Dataset')
plt.show()

# Plotting pc1 & pc2
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('PC-1', fontsize = 15)
ax.set_ylabel('PC-2', fontsize = 15)
ax.set_title('PCA on INDIAN PINES Dataset', fontsize = 20)
targets = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]
colors = ['r','g','b','y','m','c','k','r','g','b','y','m','c','k','b','r']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf['target'] == target
    ax.scatter(finalDf.loc[indicesToKeep, 'PC-1']
               , finalDf.loc[indicesToKeep, 'PC-2']
               , c = color
               , s = 9)
ax.legend(targets)
ax.grid()
plt.show() # FOR SHOWING THE PLOT

finalDf.to_csv('indian_pines_after_pca.csv')